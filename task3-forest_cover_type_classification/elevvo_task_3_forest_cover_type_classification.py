# -*- coding: utf-8 -*-
"""Elevvo Task 3: Forest Cover Type Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SUBE_udaDUP7B3RxG_57RLX9WDm8iNdy

**Import the dependencies**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.utils import resample
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
import pickle

"""**Preprocessing**"""

# load the dataset
df = pd.read_csv('/content/covtype.csv')

df.shape

df.head()

df.info()

# check for missing values
df.isnull().sum()

# check for highly correlated columns
corr = df.corr().abs()

# list highly correlated columns (r>0.9)
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))
to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]
print("Columns to drop:", to_drop)

# print cover type unique values
df['Cover_Type'].unique()

# remap cover type values to meet the GridSearchCV requirements (values should start from 0 instead of 1)
df['Cover_Type'] = df['Cover_Type'] - 1

df['Cover_Type'].unique()

"""**Split the dataset into training and testing sets**"""

# split features and target as X and y
X = df.drop(columns=['Cover_Type'])
y = df['Cover_Type']

# splitting the data into training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Model training**"""

# model training
model = XGBClassifier()
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [6, 8],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
    "reg_lambda": [1.0, 2.0],
}

# hypertuning and fitting
grid_search = RandomizedSearchCV(model, param_grid, n_iter=12, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# best model parameters
best_model = grid_search.best_estimator_
print("Best Model Parameters:", grid_search.best_params_)

"""**Model evaluation**"""

cv_scores = cross_val_score(best_model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())

# test set performance
y_pred = best_model.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# feature importance
feature_importance = best_model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
print("Feature Importance:\n", feature_importance_df)

# plot feature importance (20 most important)
k = min(20, len(feature_importance_df))
topk = feature_importance_df.head(k)

topk_for_plot = topk.iloc[::-1]

plt.figure(figsize=(10, 6))
plt.barh(topk_for_plot["Feature"], topk_for_plot["Importance"], align="center")
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title(f"Top {k} Feature Importances")
plt.tight_layout()
plt.show()